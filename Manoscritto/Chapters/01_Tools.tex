\chapter{Preliminary definitions, notation and tools}

In this chapter we will go through basic definitions and notation about tensors, introduce tensor unfoldings, see the most common tensor products and introduce the idea of tensor rank.

\section{Definitions and notation}

A \emph{tensor} is a multidimensional array. Following this simple definition, the elements of a tensor are indexed by several indices.
%%%More formally, a \N-order tensor is an element of the tensor product of \N vector spaces.
The idea of tensor generalizes those of \emph{matrix} and \emph{vector}, making it possible to naturally represent data having a multi-way origin.

\begin{Def}
  The \emph{order}, or number of \emph{ways} or \emph{modes}, of a tensor is its number of dimensions, i.e.\ its number of indices.
\end{Def}
So if we have a $d$-way tensor \A, we can refer to its elements as
\begin{equation*}
  \A(i_1,\ldots,i_d)
\end{equation*}
From this definition it\todo{Ask Laura if this form is correct} follows that vectors are tensors of order \emph{one} and matrices are tensors of order \emph{two}.

\begin{Def}
  The higher-order analogue of matrix rows and columns are called \emph{fibers}. The \emph{i-th fiber} of a tensor \A is the vector of elements given by fixing all the indices but the i-th.
\end{Def}

\begin{Def}
  Two-dimensional sections of a tensor are called \emph{slices}. A \emph{slice} is the matrix obtained by fixing all but two indices.
\end{Def}

\begin{Def}
  We can define the \emph{norm} of a tensor as a general form of the matrix Frobenius norm:
  \[
  \|\A\|_F = \sqrt{\sum_{i_1 = 1}^{I_1}\sum_{i_2 = 1}^{I_2}\cdots\sum_{i_d = 1}^{I_d}\A^2(i_1 i_2 \cdots i_d)}
  \]
\end{Def}

\todo{Vale la pena di aggiungere anche il prodotto scalare?}

%% \begin{Def}
%%   A $d$-way tensor \A is \emph{rank one} if it can be written as the outer product of $d$ vectors:
%%   \[
%%   \A = \vett{v^{(1)}} \circ \vett{v^{(2)}} \circ \cdots \circ \vett{v^{(d)}}
%%   \]
%%   This can be written elementwise as:
%%   \[
%%   \A(i_1,i_2,\cdots,i_d) = v_{i_1}^{(1)} v_{i_2}^{(2)} \cdots v_{i_d}^{(d)}
%%   \]
%% \end{Def}

\section{Unfoldings}
An important concept in the tensor world which will be heavily used in the tensor train decomposition is the operation of \emph{unfolding}\footnote{also called \emph{matricization} or \emph{flattening}}.

\emph{Unfolding} is the process of reordering the elements of a $d$-way tensor into a matrix. An \emph{unfolding matrix} of the tensor will be the result of this operation. Unfoldings can be performed using arbitrary algorithms, although some unfoldings show more useful properties than others.

In this work we will leverage the following unfolding:

\begin{Def}
  Let \A a $d$-way tensor. We define the \emph{k-th unfolding} matrix of \A as:
  \begin{equation} \label{def:unfolding}
    A_k = A_k(i_1,\dots,i_k;i_{k+1},\dots,i_d) = A(i_1,\dots,i_d)
  \end{equation}
where the first $k$ indices enumerate the rows of $A_k$, while the last $d - k$ indices enumerate the columns of $A_k$.
\end{Def}

\section{Rank-one tensor}
A $d$-way tensor \A is said to be \emph{rank-one} if it can be written as the outer product of \d vectors:
\begin{equation}
  \A = v^{(1)} \circ v^{(2)} \circ \cdots \circ v^{(d)}
\end{equation}
Elementwise this translates to:
\begin{equation}
  a_{i_1,i_2,\ldots,i_n} = v_{i_1}^{(1)} v_{i_2}^{(2)} \cdots v_{i_d}^{(d)}
\end{equation}
This can be seen as a generalization of the fact that
\begin{equation*}
  M = u v^t
\end{equation*}
is a rank-one matrix.

\section{Matrix Kronecker and Hadamard products}
\emph{Kronecker} and \emph{Hadamard} matrix products are important basic notations and they can be straighforwardly generalized to tensors. We now briefly define them as reference for the following chapters.
%\paragraph{Kronecker product}
\begin{Def}[Kronecker product]
  Let $A$ an $I \times J$ and $B$ a $K \times L$ matrices, their Kronecker product $A \otimes B$ is defined by
  \begin{equation} \label{def:kronecker}
    A \otimes B =
    \begin{bmatrix}
      a_{11}B & a_{12}B & \cdots & a_{1J}B \\
      a_{21}B & a_{22}B & \cdots & a_{2J}B \\
      \vdots & \vdots & \ddots & \cdots \\
      a_{I1}B & a_{I2}B & \cdots & a_{IJ}B
    \end{bmatrix}
  \end{equation}
\end{Def}
The result of the Kronecker product is thus an $IK \times JL$ matrix obtained by multiplying each element of $A$ by the entire matrix $B$.

%\paragraph{Hadamard product}
\begin{Def}[Hadamard product]
  The Hadamard product is the elementwise matrix product. If $A$ and $B$ are both $I \times J$ matrices, their Hadamard product $A \circ B$ is defined by
  \begin{equation} \label{def:hadamard}
    A \circ B =
    \begin{bmatrix}
      a_{11}b_{11} & a_{12}b_{12} & \cdots & a_{1J}b_{1J} \\
      a_{21}b_{21} & a_{22}b_{22} & \cdots & a_{2J}b_{2J} \\
      \vdots & \vdots & \ddots & \cdots \\
      a_{I1}b_{I1} & a_{I2}b_{I2} & \cdots & a_{IJ}b_{IJ}
    \end{bmatrix}
  \end{equation}
\end{Def}

\section{Tensor-by-matrix product}
We can define a tensor-by-matrix product in several ways. In this work we will always refer to \e{mode-k contraction} or \e{mode-k multiplication}.\todo{Forse si pu√≤ omettere?}

\begin{Def} \label{def:tensor-matrix-product}
  Given a $d$-way tensor $\A \in I_1 \times \cdots \times I_d$ with elements $\A(i_1,\ldots,i_d)$ and a matrix $U \in I_\alpha \times I_k$ with elements $U(\alpha,i_k)$, we define the \e{mode-k multiplication}
  \begin{equation*}
    \A \times_k U
  \end{equation*}
  as the contraction over the $k$-th axis\footnote{$\alpha$ is on the $k$-th place.}, yielding a tensor $\B \in I_1 \times \cdots \times I_\alpha \times \cdots \times I_d$ as follows
  \begin{equation*}
    \A \times_k U = B(i_1,\ldots,\alpha,\ldots,i_d) = \sum_{i_k = 1}^{n_k} A(i_1,\ldots,i_k,\ldots,i_d) U(\alpha,i_k)
  \end{equation*}
\end{Def}
This can be seen as performing the product of each mode-$k$ fiber by the matrix $U$.

\section{Matrix Singular Values Decomposition}
Matrices admit a very powerful decomposition format: the \emph{Singular Values Decomposition}, or \emph{SVD}. SVD is ubiquitous when working with matrices and often used as a way to obtain lower-rank matrix approximations \cite{SVDbestapprox}.

In our work SVD is used on specific tensor unfoldings, in order to build the Tensor Train representation of a tensor. Also, having a tool which in some way extends and generalizes SVD to higher dimensions is the main reason behind the development of TT and the research for other tensor decompositions in general.

Below we report the definition and some basic properties of the SVD.\todo{Maybe to be removed as it's just to push the theorem on next page...}

\begin{Teo}[Singular Values Decomposition] \label{teo:svd}
  Let $m \geq n$ and $A \in \R^{m \times n}$. Then there exist $U \in \R^{m \times n}$ and $V \in \R^{n \times n}$ orhtogonal matrices and $\Sigma \in \R^{m \times n}$ such that
  \[
  \Sigma =
  \begin{bmatrix}
    \sigma_1 &&&\\
    & \sigma_2 &&\\
    && \ddots &\\
    &&& \sigma_n
  \end{bmatrix}
  ; \qquad \text{where} \; \sigma_i \geq \sigma_j \; \forall i < j
  \]
  and that it holds $A = U \Sigma V^t$.
\end{Teo}
Diagonal elements $\sigma_i$ of $\Sigma$ are called \emph{singular values} of $A$.
\begin{Oss}
  This is, in a sense, a spectral decomposition. We can see this by looking at $A^t A$:
  \[
  A^t A = V^t \Sigma U^t U \Sigma V = V^t
  \begin{bmatrix}
    \sigma_1^2 &&&\\
    & \sigma_2^2 &&\\
    && \ddots &\\
    &&& \sigma_n^2
  \end{bmatrix}
  V
  \]
  Since $A^t A$ is symmetric, its eigenvalues $\lambda_i$ are real. If we take them in descending order, we have that $\sigma_i = \sqrt{\lambda_i}$ for all $i = 1, \ldots, n$.
\end{Oss}
\begin{Oss} \label{oss:svdrank}
  If A is rank-$k$, then all $\sigma_1, \ldots, \sigma_k$ are different from $0$, while $\sigma_{k+1}, \ldots, \sigma_n$ are equal to $0$. This again comes from noticing that $rk(A) = rk(A^t A)$ and the rank of $A^t A$ is equal to the number of its non-zero eigenvalues. From the previous remark we have that this is also the number of non-zero singular values of $A$.
\end{Oss}

When computing the SVD we may end up having, as per previous remark \eqref{oss:svdrank}, a set of zero singular values and a set of non-zero ones. However, often matrices show an entire set of non-zero singular values, with the last ones being very small and close to zero.
In this case it is often chosen to \emph{truncate} the SVD by forcing to $0$ all the singular values that are below a certain threshold. This yields a lower-rank approximation of our matrix.
This technique is used later in this work, see \eqref{cor:tt_tsvd_error}, for controlling the approximation error when constructing the Tensor Train decomposition.
%EOF
