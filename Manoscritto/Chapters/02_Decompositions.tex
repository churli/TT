\chapter{Decompositions}
Here we briefly present two important tensor decomposition mainly for their historycal interest. These were the first tensor decompositions to be developed and have helped discovering important theoretical properties of tensors, however these have relevant numerical problems that limit their suitability to real world applications.

\section{Canonical Decomposition (CANDECOMP/CP)}
The core idea behind the Canonical Decomposition\footnote{This decomposition has been historically called either Canonical Decomposition (CANDECOMP) or Parallel Factors (PARAFAC) and often abbreviated as \emph{CP}.} is to factorize a tensor into a sum of rank-one components. For example let \X an \N-th order tensor, we can factorize it as
\begin{equation}
  \X \approx \sum_{r = 1}^R a_r^{(1)} \circ a_r^{(2)} \circ \cdots \circ a_r^{(N)}
\end{equation}
where $R \in \NN$ and $a_r^{(i)} \in \RR^{I_i}$. So each of the rank-one components is built as outer product of \N vectors.

We can also express the CP decomposition in a more concise matricized form.
Let us group the vectors used in the CP to form a matrix for each mode \mate{i} as follows:
\begin{equation*}
  A^{(i)} = [ a^{(i)}_1 a^{(i)}_2 \cdots a^{(i)}_R ]
\end{equation*}
We can then use the following notation to express the CP decomposition:
\begin{equation*}
  \X \approx \llbracket A^{(1)}, A^{(2)}, \ldots, A^{(N)} \rrbracket = \sum_{r = 1}^R a_r^{(1)} \circ a_r^{(2)} \circ \cdots \circ a_r^{(N)}
\end{equation*}

\subsection{Tensor Rank}
Given a tensor \X we can define its \emph{rank} as \emph{the smallest number of rank-one tensors that generate \X as their sum}. We can see the rank as the minimum number of components in an exact CP decomposition.

The definition of rank for tensors is an analogue of the matrix rank. However there are some major differences in their properties.
The first difference is that the rank of a real-valued tensor can be different over \RR and \CC \todo{Add the example as in Kolda-Bader, here or in appendix}. Another difference is that there is no general and straightforward agorithm to determine the tensor rank\footnote{Actually it is proven in \cite{rankNP} that computing the rank of a three-dimensional tensor over any finite field is NP-complete. Over the rational numbers the problem is NP-hard.}.

\paragraph{Low-rank aproximation}
Another interesting difference between matrices and higher order tensors involves the approximation of a tensor by lower rank elements.
A well known result for matrices \cite{SVDbestapprox} shows that a best rank-k approximation is given by the leading k factors of the SVD\footnote{Here we assume the standard definition of SVD having the singular values ordered in descending magnitude}.

This does not hold for higher order tensors. For instance consider a third-order tensor of rank \R having the following CP decomposition:
\begin{equation*}
  \X = \sum_{r=1}^R a_r \circ b_r \circ c_r
\end{equation*}
We would like to be able to get a best rank-k approximation of \X by summing k factors of the CP decomposition. However this is not possible and it was shown by an example \cite{kolda2001orthogonal} where the best rank-one approximation of a cubic tensor is not a factor in the best rank-two approximation.

Tensors can be even more exciting: we can have a rank-three tensor \X arbitrarily well approximated by a sequence ${\X_k}$ of rank-two tensors. Necessarily, the best approximation lies on the boundary between the space of rank-two tensors and the space of rank-three tensors. But the the space of rank-two tensors is not closed and the sequence may converge to a rank-three tensor. \todo{Opportuna citazione qui.} %\todo{Qui rivedi bene, perchè forse è meglio metterla solo sulla storia delle successioni che convergono ad altri ranghi} %\todo{Inoltre questa roba l'ha scoperta Bini, quindi vedi un po' il suo articolo}

\section{Tucker decomposition}
The idea behind the Tucker decomposition is to decompose the input tensor into a \emph{core} tensor multiplied by a matrix along each mode.
The three-way case example, where $\X \in \R^{I \times J \times K}$ is:
\begin{equation*}
  \X \approx \G \times_1 A \times_2 B \times_3 C = \sum_{p=1}^P \sum_{q=1}^Q \sum_{r=1}^R g_{pqr} a_p \circ b_q \circ c_r
\end{equation*}
and in the elementwise form:
\begin{equation*}
  x_{ijk} \approx \sum_{p=1}^P \sum_{q=1}^Q \sum_{r=1}^R g_{pqr} a_{ip} b_{jq} c_{kr}
\end{equation*}

$P, Q$ and $R$ are the number of components (i.e.\ columns) of the factor matrices $A, B$ and $C$ respectively. When $P,Q,R$ are smaller than $I,J,K$, the core \G can be thought of as a compressed version of the input tensor \X.

The Tucker decomposition has a stable algorithm for compression but it has the relevant limitation of showing an exponential dependence on the number of dimensions $d$. The number of parameters required is $\ord(dnr+r^d)$.
While this makes the Tucker decomposition unsuitable for large $d$, this leaves enough room for valuable applications in \emph{small} dimensions, expecially in the three-dimensional case. Further in this work we will see how the Tucker decomposition can be used to further \emph{compress} the three-dimensional cores yielded by the Tensor Train decomposition.
%EOF
