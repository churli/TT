\chapter{Decompositions}
Here we briefly present two important tensor decomposition mainly for their historycal interest. These were the first tensor decompositions to be developed and have helped discovering important theoretical properties of tensors, however these have relevant numerical problems that limit their suitability to real world applications \cite{tensorreview}.

\section{Canonical Decomposition (CANDECOMP/CP)}
The core idea behind the Canonical Decomposition\footnote{This decomposition has been historically called either Canonical Decomposition (CANDECOMP) or Parallel Factors (PARAFAC) and often abbreviated as \emph{CP}.} is to factorize a tensor into a sum of rank-one components. For example let \X an \N-th order tensor, we can factorize it as
\begin{equation}
  \X \approx \sum_{r = 1}^R a_r^{(1)} \circ a_r^{(2)} \circ \cdots \circ a_r^{(N)}
\end{equation}
where $R \in \NN$ and $a_r^{(i)} \in \RR^{I_i}$. So each of the rank-one components is built as outer product of \N vectors.

We can also express the CP decomposition in a more concise matricized form.
Let us group the vectors used in the CP to form a matrix for each mode \mate{i} as follows:
\begin{equation*}
  A^{(i)} = [ a^{(i)}_1 a^{(i)}_2 \cdots a^{(i)}_R ]
\end{equation*}
We can then use the following notation to express the CP decomposition:
\begin{equation*}
  \X \approx \llbracket A^{(1)}, A^{(2)}, \ldots, A^{(N)} \rrbracket = \sum_{r = 1}^R a_r^{(1)} \circ a_r^{(2)} \circ \cdots \circ a_r^{(N)}
\end{equation*}

\subsection{Tensor Rank}
Given a tensor \X we can define its \emph{rank} as \emph{the smallest number of rank-one tensors that generate \X as their sum}. We can see the rank as the minimum number of components in an exact CP decomposition.

The definition of rank for tensors is an analogue of the matrix rank. However there are some major differences in their properties.
The first difference is that the rank of a real-valued tensor can be different over \RR and \CC
For example consider the following tensor $\A$ defined by its frontal slices as:
\begin{equation*}
  A_1 = 
  \begin{bmatrix}
    1 & 0\\
    0 & 1
  \end{bmatrix}
  ,\qquad
  A_2 = 
  \begin{bmatrix}
    0 & 1\\
    -1 & 0
  \end{bmatrix}
\end{equation*}
This is proven to have rank $3$ over $\RR$ and rank $2$ over $\CC$. Its CP rank decomposition is $\X = \llbracket A, B, C \rrbracket$ where over $\RR$
\begin{equation*}
  A = 
  \begin{bmatrix}
    1 & 0 & 1\\
    0 & 1 & -1
  \end{bmatrix}
  ,\qquad
  B = 
  \begin{bmatrix}
    1 & 0 & 1\\
    0 & 1 & 1
  \end{bmatrix}
  ,\qquad
  C = 
  \begin{bmatrix}
    1 & 1 & 0\\
    -1 & 1 & 1
  \end{bmatrix}
\end{equation*}
while over $\CC$ the factors are
\begin{equation*}
  A = \frac{1}{\sqrt{2}}
  \begin{bmatrix}
    1 & 1\\
    -i & i
  \end{bmatrix}
  ,\qquad
  B = \frac{1}{\sqrt{2}}
  \begin{bmatrix}
    1 & 1\\
    i & -i
  \end{bmatrix}
  ,\qquad
  C = 
  \begin{bmatrix}
    1 & 1\\
    i & -i
  \end{bmatrix}
\end{equation*}
Further details over this example can be found in \cite{tensorreview,kruskalstatement,ten1991kruskal,kruskal1989rank}.

Related to this, another difference between tensors and matrices lies in the fact that there is no general and straightforward agorithm to determine the tensor rank and the problem has been proven to be NP-hard\cite{rankNP}\footnote{It is proven in \cite{rankNP} that computing the rank of a three-dimensional tensor over any finite field is NP-complete. Over the rational numbers the problem is NP-hard.}.

\paragraph{Low-rank aproximation}
Another interesting difference between matrices and higher order tensors involves the approximation of a tensor by lower rank elements.
A well known result for matrices \cite{SVDbestapprox} shows that a best rank-k approximation is given by the leading k factors of the SVD\footnote{Here we assume the standard definition of SVD having the singular values ordered in descending magnitude}.

This does not hold for higher order tensors. For instance consider a third-order tensor of rank \R having the following CP decomposition:
\begin{equation*}
  \X = \sum_{r=1}^R a_r \circ b_r \circ c_r
\end{equation*}
We would like to be able to get a best rank-$k$ approximation of \X by summing some specific subset of $k$ factors of the CP decomposition. However this is not possible and it was shown in \cite{kolda2001orthogonal} by reporting a specific tensor whose best rank-one approximation of a cubic tensor is not a factor in the best rank-two approximation.

Tensors can be even more exciting: a best rank-$k$ approximation may not even exist and we can have a rank-$k$ tensor \X arbitrarily well approximated by a sequence ${\X_i}$ of rank-$s$ tensors with $s < k$. This is called \emph{degeneracy} and our rank-$k$ tensor would be called \emph{degenerate}.
The earliest example of degeneracy is from \cite{bini1980approximate} where an explicit example of a sequence of rank-$5$ tensors converging to a rank-$6$ tensor is presented.

We may encouter degeneracy when trying to find a fixed rank best aproximation of a tensor. For example, if we want to find the best rank-$2$ approximation of a rank-$3$ tensor, we can find a sequence of rank-$2$ tensors approximating the rank-$3$ tensor with increasing precision. In this case, necessarily, the best rank-$2$ approximation lies on the boundary between the space of rank-two tensors and the space of rank-three tensors. But the the space of rank-two tensors is not closed and the sequence may converge to a rank-three tensor.

More generally, we call a \emph{border tensor} a rank-$s$ tensor that admit a sequence of tensors which rank is at most $r < s$. The least value $r_b$ for which such sequence exist is called \emph{border rank} of the tensor.

\section{Tucker decomposition}
The idea behind the Tucker decomposition is to decompose the input tensor into a \emph{core} tensor multiplied by a matrix along each mode.
The three-way case example, where $\X \in \R^{I \times J \times K}$ is:
\begin{equation*}
  \X \approx \G \times_1 A \times_2 B \times_3 C = \sum_{p=1}^P \sum_{q=1}^Q \sum_{r=1}^R g_{pqr} a_p \circ b_q \circ c_r
\end{equation*}
and in the elementwise form:
\begin{equation*}
  x_{ijk} \approx \sum_{p=1}^P \sum_{q=1}^Q \sum_{r=1}^R g_{pqr} a_{ip} b_{jq} c_{kr}
\end{equation*}

$P, Q$ and $R$ are the number of components (i.e.\ columns) of the factor matrices $A, B$ and $C$ respectively. When $P,Q,R$ are smaller than $I,J,K$, the core \G can be thought of as a compressed version of the input tensor \X.

The Tucker decomposition has a stable algorithm for compression but it has the relevant limitation of showing an exponential dependence on the number of dimensions $d$. The number of parameters required is $\ord(dnr+r^d)$.
While this makes the Tucker decomposition unsuitable for large $d$, it leaves enough room for valuable applications in \emph{small} dimensions, expecially in the three-dimensional case. Further in this work we will see that the Tucker decomposition can be used to further \emph{compress} the three-dimensional cores yielded by the Tensor Train decomposition.
%EOF
