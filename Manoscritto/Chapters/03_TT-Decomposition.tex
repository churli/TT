\chapter{Tensor-Train Decomposition}

The \emph{Tensor-Train decomposition} is the main topic of this work. It was presented in this form by Oseledets in \cite{oseledets2011tt} although the format was already presented in a recursive form by the same author in \cite{oseledetstyrty2009}.

The idea behind the \emph{tensor train decomposition}, or simply \emph{TT-decomposition}, is to have a way of representing the higher order tensor using a sequence of third order tensors. This will prove to be useful, since we can use the \emph{Tucker} decomposition on these third order factors to further lower the number of parameters involved.

We define the \emph{TT-decomposition} of a given tensor \A as follows:

\begin{Def}
  Let \A a $d$-dimensional tensor. The \emph{TT-decomposition} of \A will be given by a set of third order tensors $G_1,G_2,\dots,G_d$ such as
  \begin{equation} \label{def:TT}
    A(i_1,i_2,\dots,i_d) = G_1(i_1)G_2(i_2)\cdots G_d(i_d)
  \end{equation}
  Tensor $G_1,G_2,\dots,G_d$ are called \emph{carriages} or \emph{cores} of the decomposition.
\end{Def}

Since, for each $k$, $G_k$ is a third order tensor, each $G_k(i_k)$ is a matrix slice of it. Defining each $G_k(i_k)$ as a $r_{k-1} \times r_k$ matrix and given that our original tensor \A is $n_1 \times \cdots \times n_d$, we have that each carriage $G_k$ can be treated as a $r_{k-1} \times n_k \times r_k$ tensor.

As we can see from \ref{def:TT}, the result is a $r_0 \times r_d$ matrix. Since we want to have a scalar result (i.e. the tensor element we are considering), we have to set $r_0 = r_d = 0$ as boundary condition.

The ranks $r_k$ are called \emph{compression ranks} or \emph{TT-ranks}. We will see how compression ranks can be computed and what their role is in aproximate representations.

We can now write the TT-decomposition of \A in the indexed form as:
\begin{equation} \label{def:TTindex}
  A(i_1,\dots,i_d) = \sum_{\alpha_0,\dots,\alpha_d} G_1(\alpha_0,i_1,\alpha_1) \cdots G_d(\alpha_{d-1},i_d,\alpha_d)
\end{equation}

\section{Theorical results}
We want to see now some fundamental properties of the TT-decomposition. Let \A a $d$-dimensional $n_1 \times \cdots \times n_d$ tensor. It is said to be in the TT format with cores $G_k$ of size $r_{k-1} \times n_k \times r_k$ for $k=1,\ldots ,d$, with $r_0 = r_d = 0$, if its elements are defined by \ref{def:TT}.

\begin{Teo} \label{teo:ttsvd}
  Let \A a $d$-dimensional tensor and let, for each $k$, $A_k$ its standard $k$-th unfolding matrix defined in \ref{def:unfolding}.

  If

  \[
  \forall k \quad rank(A_k) = r_k
  \]
  
  then there exists a TT-decomposition with TT-ranks not higher than $r_k$.

  \begin{proof}
    TODO\todo{In questa dimostrazione ci sono ancora dei punti poco chiari: vedila bene e semmai chiedi a Bini}
  \end{proof}
\end{Teo}

The above proof \ref{teo:ttsvd} is constructive and yields an algorithm for computing the TT-decomposition of a given tensor using $d$ sequential SVDs on the auxiliary unfolding matrices\footnote{The SVD is used to compute the dyadic decomposition of $A_k$}. This \e{full-to-TT} compression algorithm is called \e{TT-SVD}.

Since the SVD is used, we can make advantage of its properties: at each step, we can approximate $A_k$ with a lower-rank matrix via the SVD to have an approximate TT representation of the tensor. The following theorem lets us estimate the error that is introduced with this procedure:

\begin{Teo} \label{teo:ttsvd_approx}
  Suppose that the unfoldings $A_k$ of the tensor \A are rank-$k$ only approximately, i.e. they satisfy

  \begin{equation} \label{teo:ttsvd_approx:ip:1}
  A_k = R_k + E_k, \quad rank(R_k) = r_k, \quad ||E_k||_F = \epsilon_k \quad:\quad k = 1,\ldots,d-1
  \end{equation}

  Then the TT-SVD yields a tensor \B in the TT format with TT-ranks $r_k$ and

  \begin{equation}
    ||\A - \B ||_F \le \sqrt{\sum_{k=1}^{d-1} \epsilon_k^2}
  \end{equation}
  
  \begin{proof}
    This proof proceeds by induction. For $d=2$ it follows directly from the properties of the SVD. If we now consider an arbitrary $d > 2$, then the first unfolding $A_1$ can be decomposed as
    \[
    A_1 = U_1 \Sigma V_1 + E_1 = U_1 B_1 + E_1
    \]

    Now $U_1$ is $n_1 \times r_1$ and has orthonormal columns, $||E_1||_F = \epsilon_1$ and $B_1$ is naturally associated with a $(d-1)$-dimensional tensor $\B_1$ having elements $B_1(\alpha_1i_2,i_3,\ldots,i_d)$. This tensor $\B_1$ will be further decomposed in the TT-SVD algorithm: $B_1$ will be approximated by matrix $\hat{B}_1$.

    From the properties of the SVD we have $U_1^\top E_1 = 0$ \todo{Perchè? Devi approfondire e capire perchè vale questo} , thus
    \begin{equation*}
      \begin{split}
        ||\A-\B||_F^2 &= ||A_1 - U_1\hat{B}_1||_F^2 = ||A_1 - U_1(\hat{B}_1 + B_1 - B_1)||_F^2 \\
         &= ||A_1 - U_1 B_1||_F^2 + ||U_1(B_1 - \hat{B}_1)||_F^2
      \end{split}
    \end{equation*}
    and since, from the SVD definition, $U_1$ has orthonormal columns, we have
    \begin{equation} \label{teo:ttsvd_approx:proof:bound}
      ||\A - \B||_F^2 \le \eps_1^2 + ||B_1 - \hat{B}_1 ||_F^2
    \end{equation}

    The matrix $B_1$ is obtained from $A_1$ as
    \[
    B_1 = U_1^\top A_1
    \]

    If we now consider $B_{1,k}$ the $k$-th unfolding of the $(d-1)$-dimensional tensor $\B_1$, from the orthonormality of $U_1$, for each $k=2,\ldots,d-1$ we have\footnote{It's useful to recall here that the Frobenius norm is invariant under unfolding}
    \begin{equation*}
      \begin{split}
        ||B_{1,k} - R_k||_F^2 &= ||U_1^\top A_1 - R_k||_F^2 = ||A_k - R_k||_F^2\\
         &= \eps_k^2
      \end{split}
    \end{equation*}
    so the $B_{1,k}$ unfoldings satisfy \ref{teo:ttsvd_approx:ip:1} and by induction we have
    \begin{equation*}
      ||B_1 - \hat{B}_1 ||_F^2 \le \sum_{k=2}^{d-1} \eps_k^2
    \end{equation*}

    If we put this together with \ref{teo:ttsvd_approx:proof:bound} we have
    \begin{equation*}
      ||\A - \B||_F^2 \le \eps_1^2 + ||B_1 - \hat{B}_1 ||_F^2 = \sum_{k=1}^{d-1} \eps_k^2
    \end{equation*}
  \end{proof}
\end{Teo}

Two important corollaries follow from this theorem:
\begin{Cor}
  If \A admits a canonical approximation with $R$ terms \todo{Che minchia è, una CP con R addendi?} and accuracy $\eps$, then there exists a TT-approximation of \A with TT-ranks $r_k < R$ and accuracy $\sqrt{d-1} \eps$.
\end{Cor}
\begin{Cor}
  Given a tensor \A and a set of rank bounds $r_k$, the best approximation\footnote{In the Frobenius norm} $\A^{best}$ of \A with TT-ranks bounded by $r_k$ always exists and the approximation $\B$ computed by the TT-SVD agorithm is \e{quasi-optimal}, i.e.
  \begin{equation*}
    ||\A - \B||_F \le \sqrt{d-1} ||\A - \A^{best}||_F
  \end{equation*}

  \begin{proof}
    Let
    \begin{equation*}
     \eps = \inf_{\tensor{C} \in TT(r_k)} ||\A - \tensor{C}||_F
    \end{equation*}
    where $TT(r_k)$ is the set of all the tensor trains with TT-ranks bounded by $r_k$. By definition of infimum there exists a sequence of tensor trains $\B^{(s)} : s \in \NN$ such that $lim_{s\to\infty} ||\A - \B||_F = \eps$.

    The elements of $\B^{(s)}$ are bounded and \todo{Perchè averli bounded ci serve?} there is some subsequence $\B^{(s_t)}$ converging elementwise to a tensor $\B^{min}$. Their unfolding matrices converge too. Now, for each $k$, the set of matrices having rank not greater than by $r_k$ is closed and since $\B^{(s_t)} \in TT(r_k)$
    \begin{equation*}
      rank(\B^{(s_t)}_k) \le r_k \quad \forall k
    \end{equation*}
    and thus
    \begin{equation*}
      rank(\B^{min}_k) \le r_k \quad \forall k
    \end{equation*}
    Moreover, $||\A - \B^{min}||_F = \eps$, \todo{Perchè questo, bisogna controllare!} so $\B^{min}$ is the minimum.

    Finally, we note that $\eps_k \le \eps$, since each unfolding can be approximated at least with accuracy $\eps$, and the quasi-optimality bound follows directly from \ref{teo:ttsvd_approx}
  \end{proof}
\end{Cor}

\begin{Cor}
  From \ref{teo:ttsvd_approx} it also follows that if the SVD of the unfoldings are truncated at $\delta$, the approximation error of the whole TT will be $\delta \sqrt{d-1}$. So to obtain any prescribed accuracy $\eps$ on the TT whe have to set the treshold to $\delta = \frac{\eps}{d-1}$
\end{Cor}

\subsection{How many parameters?}
Let's take an $n_1 \times \cdots \times n_d$ tensor $\A \in I_1 \times \cdots \times I_d$. The number of parameters used to represent it in the full format is $n_1 \cdots n_d$. If we assume , $\forall k$, $n_k  = n$, we have an $n^d$ number of parameters used.

If \A is instead represented in the TT format, as in \ref{def:TTindex}, with TT-ranks $r_k$ we have that the number of parameters used is
\begin{equation*}
  \sum_{k=1}^d r_{k-1} n_k r_k
\end{equation*}
Since we know that $r_0 = r_d = 1$ and assuming that, $\forall k$, $r_k = r$, we have the following estimate
\begin{equation*}
  %n_1 r_1 + r_{d-1} n_d + \sum_{k=2}^{d-1} r_{k-1} n_k r_k =
  2nr + (d-2)nr^2
\end{equation*}

Using a Tucker decomposition on each of the three-dimesional TT-cores this can be lowered further to
\begin{equation*}
  dnr + (d-2)r^3
\end{equation*}

\section{Rounding in the TT format}
We have seen how a full tensor can be converted into the TT format using the TT-SVD algorithm. 
