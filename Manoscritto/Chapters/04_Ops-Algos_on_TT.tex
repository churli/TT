\chapter{Basic linear algebra on TT}
Now we will see how some basic operations can be performed on the TT format and what their complexity is.

\section{Addition and multiplication by a number}
Let \A and \B $d$-dimensional tensors that can be written in the TT format as
\begin{equation*}
  \begin{split}
    \A &= G_1(i_1) \cdots G_d(i_d)\\
    \B &= H_1(i_1) \cdots H_d(i_d)\, .
  \end{split}
\end{equation*}

\paragraph{Multiplication by a number.}
This is a very easy operation, since to scale a tensor by $\alpha$ it is sufficient to scale one of the cores by it.

\paragraph{Addition.}
The addition of these two tensors can be performed by \emph{merging} each couple of cores, i.e., the sum $\C = \A + \B$ will have \e{central} cores
\begin{equation*}
  J_k(i_k) = 
  \begin{pmatrix}
    G_k(i_k) & 0\\
    0 & H_k(i_k)
  \end{pmatrix}
\end{equation*}
and \e{border} cores
\begin{equation*}
  \begin{split}
    J_1(i_d) &= 
  \begin{pmatrix}
    G_1(i_1) & H_1(i_1)
  \end{pmatrix}\\
  J_d(i_d) &= 
  \begin{pmatrix}
    G_d(i_d)\\
    H_d(i_d)
  \end{pmatrix}\, .
  \end{split}
\end{equation*}

We can easily prove the above by direct multiplication:
\begin{equation*}
  J_1(i_1) \cdots J_d(i_d) = G_1(i_1) \cdots G_d(i_d) + H_1(i_1) \cdots H_d(i_d)\, .
\end{equation*}

By the definition of the cores, we can see that summing tensors leads to a TT-rank increase. For each core of the sum, we have a rank that is the sum of the ranks of the underlying cores. This makes the addition operation a good test bench for the rounding procedure.

\section[Contraction, products and norm]{Multidimensional contraction, Hadamard product, scalar product, Frobenius norm}
\paragraph{Multidimensional contraction.}
The multidimensional contraction is the evaluation of an expression of the form
\begin{equation*}
  W = \sum_{i_1,\ldots,i_k} A(i_1,\ldots,i_d) u_1(i_1) \cdots u_d(i_d)
\end{equation*}
where, for each $k$, $u_k(i_k)$ is an $n_k$-long vector. This can be seen as a scalar product with a rank-one tensor
\begin{equation*}
  W = \langle \A, \otimes_{i=1}^d u_j \rangle\, .
\end{equation*}

If \A is in the TT format
\begin{equation*}
  \begin{split}
    \A &= G_1(i_1) \cdots G_d(i_d)
  \end{split}
\end{equation*}
then the multidimensional contraction can be seen as
\begin{equation*}
  W = \Bigg( \underbrace{\sum_{i_1} u_1(i_1)G_1(i_1)}_{\Gamma_1} \Bigg) \Bigg( \underbrace{ \sum_{i_2} u_2(i_2)G_2(i_2)}_{\Gamma_2} \Bigg) \cdots \Bigg( \underbrace{\sum_{i_d} u_d(i_d)G_d(i_d)}_{\Gamma_d} \Bigg)\, .
\end{equation*}
The matrix $\Gamma_k$ is $r_{k-1} \times r_k$, so $\Gamma_1$ is a row vector and $\Gamma_d$ is a column vector. Computing $W$, the multidimensional contraction, means thus computing matrices $\Gamma_k$ and evaluating $d$ matrix-by-vector products, requiring a total of $\ord(dnr^2)$ arithmetic operations.

\paragraph{Hadamard product.} The Hadamard product, defined in \eqref{def:hadamard}, can be computed in the TT format as follows.

Consider $\C = \A \circ \B$: from the definition of its elements we can easily derive the way to compute the TT-cores associated with this product
\begin{equation*}
  \begin{split}
    C(i_1,\ldots,i_d) &= A(i_1,\ldots,i_d) B(i_1,\ldots,i_d)\\
    &= \Big( A_1(i_1) \cdots A_d(i_d) \Big) \otimes \Big( B_1(i_1) \cdots B_d(i_d) \Big) \\
    &= \Big( A_1(i_1) \otimes B_1(i_1) \Big) \cdots \Big( A_d(i_d) \otimes B_d(i_d) \Big)\, .
  \end{split}
\end{equation*}

\paragraph{Scalar product.}
The scalar product of two tensors can be computed using the Hadamard product and the multidimensional contraction. For two tensors \A and \B it is defined as
\begin{equation*}
  \langle \A, \B \rangle = \sum_{i_1,\ldots,i_d} A(i_1,\ldots,i_d)B(i_1,\ldots,i_d) = \sum_{i_1,\ldots,i_d} \A \circ \B\, .
\end{equation*}
Looking at the complexity, we have that the ranks of the product are $\ord(r^2)$ and computing the contraction with vectors of all ones gives $\ord(dnr^4)$.
However, this complexity can be reduced by exploiting the way the contraction is computed in the TT format. We have seen that the contraction is reduced to the computation of the product
\begin{equation*}
  W = \Gamma_1 \cdots \Gamma_d
\end{equation*}
where
\begin{equation*}
  \Gamma_k = \sum_{i_k}A_k(i_k) \otimes K_k(i_k)\, .
\end{equation*}
Since $\Gamma_1$ is a row vector, the product can be computed with a sequence of vector-by-matrix products:
\begin{equation*}
  \begin{split}
    v_1 &= \Gamma_1\\
    v_k &= v_{k-1} \Gamma_k, \quad \forall k = 2,\ldots,d\, .
  \end{split}
\end{equation*}
Each $v_k$ is a row vector with length $r_k^{(A)} r_k^{(B)}$. If we now look at the computation of $v_k$, given that $v_{k-1}$ is known, we have:
\begin{equation*}
  v_k = v_{k-1} \Gamma_k = v_{k-1} \sum_{i_k} A_k(i_k) \otimes B_k(i_k) = \sum_{i_k} \underbrace{v_{k-1} \Big( A_k(i_k) \otimes B_k(i_k) \Big) }_{p_k(i_k)}\, .
\end{equation*}
Each $p_k(i_k)$ is again a row vector with length $r_k^{(A)} r_k^{(B)}$.
Assuming now that all the TT-ranks involved have order $r$, we have, thanks to the special structure of the Kronecker product, that the computation of each $p_k(i_k)$ can be done in $\ord(r^3)$ operations. Thus the complete scalar product costs $\ord(dnr^3)$.

\paragraph{Frobenius norm.}
The Frobenius norm can be computed directly as
\begin{equation*}
  ||\A||_F = \sqrt{\langle \A, \A \rangle}
\end{equation*}
and, by consequence, the distance between two tensors
\begin{equation*}
  ||\A - \B||_F\, .
\end{equation*}
Deriving from the scalar product, the complexity for the computation of the Frobenius norm is $\ord(dnr^3)$.
%\todo{Si fa anche il matrix-by-vector product?}

%EOF
