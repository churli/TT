\chapter{TT-Cross}
So far we have seen how a given tensor can be:
\begin{itemize}
\item compressed from the \e{full format} to the TT format
\item manipulated via various operations directly into the TT format
\item \e{rounded} (i.e.\ recompressed) without leaving the TT format
\end{itemize}

However, we still have a very relevant dependency on either the full representation format or the canonical decomposition as unique ways to input data to the TT format.
In fact, in case of a high value of $d$, we likely may be unable to represent the full tensor in order to perform the compression at the first instance. On the other hand, a canonical representation of tensors is usually already available only in few special cases (e.g.\ for discrete operators).

Fortunately, we can notice that, in all analytical problems and in many practical applications, tensors are often described implicitly by a procedure letting us compute any of its elements.
This is what is called a \e{black-box} tensor.

Having to deal with a black-box tensor can result in a relevant advantage: we can compute any of its element given their coordinates, thus we don't have to store the full multi-dimensional array of elements in order to be able to access them.

The question we want to answer in this chapter is: \e{Can we exploit this advantage? Can we find a way to have an arbitrary good approximation of the tensor just by using a limited subset of its element?}

\paragraph{Tensor train cross-approximation}
Here we are going to describe a low-rank tensor approximation technique using a subset of few, well-chosen, elements.
The fundamental idea, described in \cite{tt-cross}, is to extend the dyadic (skeleton) decomposition for matrices to higher dimensions using the TT format.
The outcoming algorithm is called \e{TT-cross} and it is shown that, if TT ranks are bounded by $r$, then only $\ord(dnr^2)$ carefully-chosen elements are required in order to completely recover the tensor.

\section{Skeleton decomposition for matrices}
First of all, we need to introduce the skeleton decomposition for the exact matrix case. We are not going to give any proof of its properties here and the interested reader can find them on \cite{goreinov1997theory,gantmacher1959matrix}.

\begin{Teo}
  Let $A$ an $m \times n$ matrix with rank $r$. $A$ can be represented as
  \begin{equation} \label{def:skeleton}
    A = C \hat{A}^{-1} R
  \end{equation}
  where:
  \begin{itemize}
  \item $C = A(:,\J)$ is made of a subset of $r$ columns of $A$, indexed by $\J$
  \item $R = A(\I,:)$ is made of a subset of $r$ rows of $A$, indexed by $\I$
  \item $\hat{A} = A(\I,\J)$ is the submatrix on their intersection, that should be nonsingular
  \end{itemize}
\end{Teo}

From this it follows that a rank-$r$ matrix can be recovered from $r$ linearly independent rows and $r$ linearly independent columns\footnote{The total amount of elements that have to be stored for a rank-$r$, $m \times n$ matrix, is $r(m+n-r)$.}.

\paragraph{An approximate skeleton}
We want now to use this skeleton decomposition in the case where the matrix $A$ is only approximately of rank $r$, e.g.\ when its $\eps$-rank is $r$.
How should we choose the $r$ rows and $r$ colums to use? Is there a way yielding better results than others?

An answer is to pick such rows and columns in a way that their intersection submatrix has maximal volume (i.e.\ determinant in modulus) \cite{goreinov2001maximal}. We can also leverage cross-approximation techniques allowing us to compute quasi-optimal sets of rows and columns for this purpose \cite{tyrtyshnikov2000incomplete,bebendorf2000approximation}. The \e{maxvol} algorithm \cite{goreinov2010find} is what is currently used in the implementation of the TT-cross algorithm.

The high-level strategy that is used with matrices is the following.

First choose a subset of either rows or columns containing a submatrix of sufficiently large volume. E.g. consider a set $\J = [i_k], k=1,\ldots,r$ of column indices suitable for this purpose.

Knowing $\J$, we can compute the elements of the $n \times r$ column matrix $C = A(:,\J)$. This obviously takes $nr$ element evaluations.
Using the \e{maxvol} algorithm we can then obtain a submatrix $\hat{A}$ of quasi-maximal volume in $C$ with $\ord(nr^2)$ operations.
Since the previous step also gives us the set of row indices containing $\hat{A}$, the row matrix can now be computed as $R = A(\I,:)$.

\section{From skeletons to trains}
Now that we have seen the skeleton decomposition for matrices, how can we use it to compute a \e{tensor-train} approximation?
The basic idea is to replace the SVD with the skeleton decomposition in the proof of Theorem \ref{teo:ttsvd} and therefore to adapt the \e{TT-SVD} compression algorithm in order to leverage the properties of the skeleton decomposition.

Considering again the strategy described above for matrices, we can notice that only one set of indices, either $\J$ or $\I$, is required at the beginning. This is going to be very helpful in the tensor case.

\paragraph{TT-cross}
Given a tensor $\A = A(i_1,\ldots,i_d)$, let us consider the first unfolding matrix $A_1$ and its rank $r_1$. We know that there is a subset of $r_1$ linearly independent columns in $A_1$, so let us take $\J_1$ as the set of their indices.

Since each column of $A_1$ is naturally pointed to a multi-index $(i_2,\ldots,i_d)$, $\J_1$ is actually a set of $(d-1)$-tuples and can be written as:
\begin{equation*}
  \J_1 = [j_l^{(\alpha_1)}], \quad \alpha_1 = 1,\ldots,r_1, \quad l = 2,\ldots,d
\end{equation*}
where $\alpha_1$ is the column number and $l$ is the mode, so each of the $r_1$ columns is split in $(d-1)$ mode-indices.

The unfolding $A_1$ has a skeleton decomposition where the column matrix $C$ has size $n_1 \times r_1$ and is made of $r_1$ columns of $A_1$.
It can be computed as
\begin{equation*}
  C(i_1,\alpha_1) = A(i_1,j_2^{(\alpha_1)},\ldots,j_d^{(\alpha_1)})\, .
\end{equation*}

Using the \emph{maxvol} algorithm we can then compute a submatrix of $C$ having quasi-maximal volume, $\hat{A}_1$. It will also yield the corresponding row-indices set
\begin{equation*}
  \I_1 = [I_1^{(\alpha_1)}], \quad \alpha_1 = 1,\ldots,r_1\, .
\end{equation*}

Matrix $A_1$ is now represented as
\begin{equation*}
  A_1 = C\hat{A}_1^{-1}R
\end{equation*}
and similarly to the TT-SVD case, we can set the first tensor carriage as
\begin{equation*}
  G_1 = C\hat{A}_1^{-1}\, .
\end{equation*}

We can now notice that, if $\hat{A}_1$ is of maximal volume in $C$, then $G_1$ has elements not higher than 1 in modulus. This property on the elements of $G_1$ holds when $\hat{A}_1$ is the output of \emph{maxvol} \cite{tyrtyshnikov2000incomplete,goreinov2010find}.

The next step is to iterate further on the $R$ term and represent it in the TT format. $R$ consists of $r_1$ rows of $A_1$ and it can be reshaped into a subtensor $\tensor{R}$ of $\A$:
\begin{equation*}
  R(\alpha_1,i_2,\ldots,i_d) = A(i_1^{(\alpha_1)},i_2,\ldots,i_d)\, .
\end{equation*}
We can now concatenate indices $\alpha_1$ and $i_2$ in one long index $\alpha_1i_2$: in this way $\tensor{R}$ can be seen as a tensor of dimensionality $d-1$ and size $r_1n_2 \times n_2 \times \cdots \times n_d$.

Similarly to as shown in proof of Theorem \ref{teo:ttsvd}, if the unfoldings $A_k$ have ranks $r_k$, then the corresponding compression ranks for $\tensor{R}$ cannot be larger and, in case the TT representation of $\A$ is exact, these compression ranks must be equal to $r_k$.

In order to obtain the second carriage, we consider again the first unfolding of the new, reshaped, subtensor \tensor{R}:
\begin{equation*}
  R_2 = R(\alpha_1i_2;i_3,\ldots,i_d)
\end{equation*}

%Page 79 of article
Now suppose that we know a set
\begin{equation*}
  \J_2 = [j_l^{(\alpha_2)}], \quad \alpha_2 = 1,\ldots,r_2, \quad l = 2,\ldots,d
\end{equation*}
of $(d-2)$-tuples, indicating the columns containing a submatrix of quasi-maximal volume in $R_2$. In order to obtain the skeleton decomposition of $R_2$, we use these linearly independent columns to make matrix $C_2$ of size $r_1 n_2 \times r_2$. Computing its entries
\begin{equation*}
  C_2(\alpha_1 i_2; \alpha_2) = R(\alpha_1 i_2,j_3^{(\alpha_2)},\ldots,,j_d^{(\alpha_2)})
\end{equation*}
that, in terms of the initial array are
\begin{equation*}
  C_2(\alpha_1 i_2; \alpha_2) = R(i_1^{(\alpha_1)},i_2,j_3^{(\alpha_2)},\ldots,j_d^{(\alpha_2)})
\end{equation*}
where the indices $i_1^{(\alpha_1)}$ belong to set $\I_1$ computed at the previous step.

As before, we find the rows containing a quasi-maximal volume submatrix in $C_2$ and we construct the skeleton decomposition by using $C_2$ and some row matrix with $r_2$ rows. Again this row matrix gives rise to a new subtensor of size $r_2n_3 \times n_4 \times n_d$, which plays the same role and will be handled in the same way as $\tensor{R}$.

The row indices that define the quasi-maximal volume submatrix in $C_2$ are chosen among the values of the long indices $\alpha_1i_2$ where $\alpha_1$ corresponds to \emph{one-dimensional} indices in $\I_1$. These can thus be considered as long indices of the form $i_1^{(\alpha_2)}i_2^{(\alpha_2)}$ and can be generalized straightforwardly to all further unfoldings.

At the $k$-th step we have a subtensor $\tensor{R}$ of dimensionality $d-k+1$ and size $r_{k-1}n_k \times n_{k+1} \times \ldots \times n_d$. Its definition as subset of entries of the initial tensor $\A$ is the following:
\begin{equation*}
  R(\alpha_{k-1}n_k,n_{k+1},\ldots,n_d) = A(i_1^{(\alpha_{k-1})},i_2^{(\alpha_{k-1})},\ldots,i_{k-1}^{(\alpha_{k-1})},i_k,i_{k+1},\ldots,i_d)\, .
\end{equation*}
The first $k-1$ indices, which we call \emph{left indices}, are taken as $(k-1)$-tuples from the already computed entries of the \emph{left index set}
\begin{equation*}
  \I_{k-1} = [i_s^{(\alpha_{k-1})}],\quad \alpha_{k-1} = 1,\ldots,r_{k-1},\quad s = 1,\ldots,k-1\, .
\end{equation*}
The first unfolding of the current subtensor $\tensor{R}$ at the $k$-th step is
\begin{equation*}
  R_k = [R(\alpha_{k-1}i_k;i_{k+1},\ldots,i_d)]\, .
\end{equation*}
Again, columns containing a submatrix of quasi-maximal volume are supposed to be known and taken from the set of $(d-k)$-tuples
\begin{equation*}
  \J_k = [j_l^{(\alpha_k)}],\quad \alpha_k = 1,\ldots,r_k,\quad l = k+1,\ldots,d\, .
\end{equation*}
Obviously these columns contain a submatrix $C_k$ of size $r_{k-1}n_k \times r_k$ with entries
\begin{equation*}
  C_k(\alpha_{k-1}i_k,\alpha_k) = A(i_1^{(\alpha_{k-1})},i_2^{(\alpha_{k-1})},\ldots,i_{k-1}^{(\alpha_{k-1})},i_k,j_{k+1}^{(\alpha_k)},\ldots,j_d^{(\alpha_k)})\, .
\end{equation*}
Now the next \emph{left index set} is defined by the row positions of the quasi-maximal volume submatrix in $C_k$, which we can now denote as $\hat{R}^{-1}$.

The new tensor carriage $\G_k$ is then obtained by reshaping the result of $C_k \hat{R}^{-1}$.

\paragraph{Recap}
Let's now recap the entire procedure. If we know the index sets $\J_k$
\begin{equation*}
  \J_k = [j_l^{(\alpha_k)}],\quad \alpha_k = 1,\ldots,r_k,\quad l = k+1,\ldots,d
\end{equation*}
corresponding to the columns that contain quasi-maximal submatrices in the unfolding matrices $A_k$ and if the values $r_k = rank(A_k)$ are the compression ranks for the TT decomposition we are trying to obtain, then then number of elements fo $\A$ that are required in order to compute such TT decomposition is
\begin{equation*}
  n_1 r_1 + n_2 r_1 r_2 + \ldots + n_{d-1} r_{d-2} r_{d-1} + n_d r_{d-1}\, .
\end{equation*}
This means that if $n_k = n$ and $r_k = r$ for all $k$, then the number of elements that have to be computed are $\ord(dnr^2)$.
In addition to this, we also have to perform $d$ searches for a quasi-maximal volume submatrix in matrices that have size $nr \times r$. Using the \emph{maxvol} algorithm we can achieve this is $\ord(dnr^3)$ operations.
As a side note, we should also add that usually evaluating a function in $d$ variables takes at least $\ord(d)$ operations, which would make the whole method scale quadratically in $d$. In case of a very large number of dimensions $d$ this could be problematic and it would be useful to find and use a specific fast method for computing the subtensor
\begin{equation*}
  C_k = A(i_1^{(\alpha_{k-1})},i_2^{(\alpha_{k-1})},\ldots,i_{k-1}^{(\alpha_{k-1})},i_k,j_{k+1}^{(\alpha_k)},\ldots,j_d^{(\alpha_k)})\, .
\end{equation*}

The computation of the TT decomposition's carriages also yields the \emph{left index sets} $\I_k$. As opposed to the \emph{right index sets} $J_k$, which are assumed to be given at the beginning, these are not arbitrary and depend on each other. Each $\I_k$ is of the form
\begin{equation*}
  \I_k = [i_k^{(\alpha_k)}], \quad \alpha_k = 1,\ldots,k \quad k = 1,\ldots,d-1
\end{equation*}
and relates to $\I_{k-1}, \forall k \geq 2$ as
\begin{equation*}
  (i_1,i_2,\ldots,i_{k-1},i_k) \in \I_k \implies (i_1,i_2,\ldots,i_{k-1}) \in \I_{k-1}, \quad k=2,\ldots,d-1\, .
\end{equation*}
A sequence matching this definition will be called a \emph{left-nested sequence}.

A similar definition can be given for a \emph{right-nested sequence} of right-index sets $\J_k$, although in the above construction it was not assumed for the sequence $\J_k$:
\begin{equation*}
  (j_{k+1},j_{k+2},\ldots,j_d) \in \J_k \implies (j_{k+2},\ldots,j_d) \in \J_{k+1}, \quad k=1,\ldots,d-2\, .
\end{equation*}

With the above we have proven the following theorem:
\begin{Teo}
  Let $\A$ be an arbitrary $d$-dimensional tensor of size $n_1 \times \cdots \times n_d$ having compression ranks
  \begin{equation*}
    r_k = rank(A_k), \quad A_k = A(i_1 i_2 \ldots i_k;i_{k+1} \ldots i_d)\, .
  \end{equation*}
  Assume that right-index sets
  \begin{equation*}
    \J_k = [j_l^{(\beta_l)}], \quad \beta_l = 1,\ldots,r_k, \quad l = k,\ldots,d-1
  \end{equation*}
  and left index sets
  \begin{equation*}
    \I_k = [i_l^{(\alpha_l)}], \quad \alpha_l = 1,\ldots,r_k, \quad l = 1,\ldots,k-1
  \end{equation*}
  are given such that
  \begin{itemize}
  \item the left index sets form a left-nested sequence
  \item the $r_k \times r_k$ intersection matrices
    \begin{equation*}
      \hat{A}_k(\alpha_k,\beta_k) = A(i_1^{(\alpha_k)}, \ldots, i_k^{(\alpha_k)}; j_k^{(\beta_k)}, \ldots, j_d^{(\beta_k)}), \quad \alpha_k,\beta_k = 1,\ldots,r_k
    \end{equation*}
    are all non-singular
  \end{itemize}

  Then \A can be recovered from three-dimensional $r_{k-1} \times n_k \times r_k$ tensors $\C_k$ having elements
  \begin{equation*}
    C_k(\alpha_k,i_k,\beta_k) = A(i_1^{(\alpha_k)},\ldots,i_{k-1}^{(\alpha_k)},i_k,j_{k-1}^{(\beta_k)},\ldots,j_d^{(\beta_k)})
  \end{equation*}
  by using the tensor-train interpolation formula
  \begin{align*}
    A(i_1,\ldots,i_d) =& \sum_{\alpha_1,\ldots,\alpha_{d-1}} \left[ \, \hat{C}_1(i_1,\alpha_1) \hat{C}_2(\alpha_1,i_2,\alpha_2) \quad \cdots \right. \nonumber \\
    &\cdots \quad \left. \hat{C}_{d-1}(\alpha_{d-2},i_{d-1},\alpha_{d-1}) \hat{C}_d(\alpha_{d-1},i_d) \, \right]
  \end{align*}
  where tensor carriages $\hat{\C}_k$ are obtained from $\C_k$ by the following scaling.\footnote{The general $k$-th tensor-by-matrix multiplication is defined in \ref{def:tensor-matrix-product}. Here computing $\C_k \times_3 \hat{A}_{-1}$ is equivalent to taking the second unfolding of $\C_k$, computing the product $\hat{\C}_k(\alpha_{k-1},i_k;\alpha_k) = \C_k(\alpha_{k-1},i_k;\alpha_k) \cdot \hat{A}_k^{-1}$ and reshaping the result back to a 3-way tensor $\hat{C}_k = \hat{\C}_k(\alpha_{k-1},i_k,\alpha_k)$.}:
  \begin{align*}
    \hat{\C}_1 &= \C_1 \hat{A}_1^{-1} \nonumber \\
    \hat{\C}_k &= \C_k \times_3 \hat{A}_k^{-1}, \quad k = 2, \ldots, d-1 \\
    \hat{\C}_d &= \C_d \nonumber
  \end{align*}
\end{Teo}

It is important to note that this theorem is formally valid only in the exact low-rank case. The choice of using quasi-maximal volume submatrices was not necessay in the above proof and the only important factor was having all the intersection submatrices non-singular.

Moving to a numerically viable algorithm requires a careful selection of \emph{sufficiently good} submatrices among those that are non-singular.

Also, a practical algorithm involves skeleton approximations of a given rank, rather than exact decompositions. We have to note that there is currently no formal result giving a bound on the TT-approximation error based on approximation errors on the unfoldings given by performing approximate skeleton decompositions. However, there is experimental evidence \cite{tt-cross} suggesting this approach has good stability properties.

% \section{A TT-cross algorithm}
% \todo{Write something here from page 12, tt-cross paper \cite{tt-cross}}
\begin{lstlisting}[float, caption=Low-rank approximation algorithm, label=algo:cross-approx, title=Low-rank approximation algorithm]
  # $A$ is a $n \times m$ matrix
  # $r$ is the target rank upper bound
  # $\delta$ is the stopping accuracy parameter
  lowRankApprox($A$, $r$, $\delta$)
    # Initialization
    $\J = 1, \ldots, r$, $A_0 = 0_{n \times m}$, $k = 0$
    do
      # Row cicle
      $R = A(:,\J)$
      # Compute QR
      $R = QT$, with $Q$ a $n \times r$ matrix
      # Now get the column indices of the quasi-maximal volume submatrix
      $\I$ = maxvol($Q$)
      # Column cycle
      $C = A(\I,:)$
      $C = C^T$, so now $C$ is $m \times r$
      # Compute QR
      $C = QT$, with $Q$ a $m \times r$ matrix
      # Now get the row indices of the quasi-maximal volume submatrix
      $\J$ = maxvol($Q$)
      # Compute the new approximation
      $\hat{Q} = Q(\J,:)$
      $A_{k+1} = A(:,\J)(Q\hat{Q}^{-1})^T$
      $k = k+1$
    while $||A_k - A_{k-1}||_F > \delta ||A_k||_F$
    return $A(\I,:)$, $\J$
\end{lstlisting}

%EOF
