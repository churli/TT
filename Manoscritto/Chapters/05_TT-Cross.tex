\chapter{TT-Cross}
So far we have seen how a given tensor can be:
\begin{itemize}
\item compressed from the \e{full format} to the TT format
\item manipulated via various operations directly into the TT format
\item \e{rounded} (i.e. recompressed) without leaving the TT format
\end{itemize}

However, we still have a very relevant dependency on either the full representation format or the canonical decomposition as unique ways to input data to the TT format.
In fact, in case of a high value of $d$, we likely may be unable to represent the full tensor in order to perform the compression at the first instance. On the other hand, a canonical representation of tensors is usually already available only in few special cases (e.g. for discrete operators).

Fortunately, we can notice that, in all analytical problems and in many practical applications, tensors are often described implicitly by a procedure letting us compute any of its elements.
This is what is called a \e{black-box} tensor.

Having to deal with a black-box tensor can result in a relevant advantage: we can compute any of its element given their coordinates, thus we don't have to store the full multi-dimensional array of elements in order to be able to access them.

The question we want to answer in this chapter is: \e{Can we exploit this advantage? Can we find a way to have an arbitrary good approximation of the tensor just by using a limited subset of its element?}

\paragraph{Tensor train cross-approximation}
Here we are going to describe a low-rank tensor approximation techinique using a subset of few, well-chosen, elements.
The fundamental idea, described in \cite{tt-cross}, is to extend the dyadic (skeleton) decomposition for matrices to higher dimensions using the TT format.
The outcoming algorithm is called \e{TT-cross} and it is shown \todo{da dimostrare? o forse Ã¨ meglio mettere 'it turns out'?} that, if TT ranks are bounded by $r$, then only $\ord(dnr^2)$ carefully-chosen elements are required in order to completely recover the tensor.

\section{Skeleton decomposition for matrices}
First of all, we need to introduce the skeleton decomposition for the exact matrix case. We are not going to give any proof of its properties here and the interested reader can find them on [???] \todo{Vai in biblioteca e spera di trovare questo \emph{Grantmacher-Theory of Matrices}, che su internet non si trova nulla}.

\begin{Teo}
  Let $A$ an $m \times n$ matrix with rank $r$. $A$ can be represented as
  \begin{equation} \label{def:skeleton}
    A = C \hat{A}^{-1} R
  \end{equation}
  where:
  \begin{itemize}
  \item $C = A(:,\J)$ is made of a subset of $r$ columns of $A$, indexed by $\J$
  \item $R = A(\I,:)$ is made of a subset of $r$ rows of $A$, indexed by $\I$
  \item $\hat{A} = A(\I,\J)$ is the submatrix on their intersection, that should be nonsingular
  \end{itemize}
\end{Teo}

From this it follows that a rank-$r$ matrix can be recovered from $r$ linearly independent rows and $r$ linearly independent columns\footnote{The total amount of elements that have to be stored for a rank-$r$, $m \times n$ matrix, is $r(m+n-r)$.}.

\paragraph{An approximate skeleton}
We want now to use this skeleton decomposition in the case where the matrix $A$ is only approximately of rank $r$, e.g. when its $\eps$-rank is $r$.
How should we choose the $r$ rows and $r$ colums to use? Is there a way yielding better results than others?

An answer is to pick such rows and columns in a way that their intersection submatrix has maximal volume (i.e. determinant in modulus) \todo{Refs here?}. There are cross-approximation techniques allowing us to compute quasi-optimal sets of rows and columns for this purpose. The \e{maxvol} \todo{References on maxvol to be added here!} algorithm is what is currently used in the implementation of the TT-cross algorithm.

The high-level strategy that is used with matrices is the following.

First choose a subset of either rows or columns containing a submatrix of sufficiently large volume. E.g. consider a set $\J = [i_k], k=1,\ldots,r$ of column indices suitable for this purpose.

Knowing $\J$, we can compute the elements of the $n \times r$ column matrix $C = A(:,\J)$. This obviously takes $nr$ element evaluations.
Using the \e{maxvol} algorithm we can then obtain a submatrix $\hat{A}$ of quasi-maximal volume in $C$ with $\ord(nr^2)$ operations.
Since the previous step also gives us the set of row indices containing $\hat{A}$, the row matrix can now be computed as $R = A(\I,:)$.

\section{From skeletons to trains}
Now that we have seen the skeleton decomposition for matrices, how can we use it to compute a \e{tensor-train} approximation?
The basic idea is to replace the SVD with the skeleton decomposition in the proof of \ref{teo:ttsvd} and therefore to adapt the \e{TT-SVD} compression algorithm in order to leverage the properties of the skeleton decomposition.

Considering again the strategy described above for matrices, we can notice that only one set of indices, either $\J$ or $\I$, is required at the beginning. This is going to be very helpful in the tensor case.

\paragraph{TT-cross}
Given a tensor $\A = A(i_1,\ldots,i_d)$, let us consider the first unfolding matrix $A_1$ and its rank $r_1$. We know that there is a subset of $r_1$ linearly independent columns in $A_1$, so let us take $\J_1$ as the set of their indices.

Since each column of $A_1$ is naturally pointed to a multi-index $(i_2,\ldots,i_d)$, $\J_1$ is actually a set of $(d-1)$-tuples and can be written as:
\begin{equation*}
  \J_1 = [j_l^{(\alpha_1)}], \quad \alpha_1 = 1,\ldots,r_1, \quad l = 2,\ldots,d
\end{equation*}
where $\alpha_1$ is the column number and $l$ is the mode, so each of the $r_1$ columns is split in $(d-1)$ mode-indices.

The unfolding $A_1$ has a skeleton decomposition where the column matrix $C$ has size $n_1 \times r_1$ and is made of $r_1$ columns of $A_1$.
It can be computed as
\begin{equation*}
  C(i_1,\alpha_1) = A(i_1,j_2^{(\alpha_1)},\ldots,j_d^{(\alpha_1)})
\end{equation*}

Using the \emph{maxvol} algorithm we can then compute a submatrix of $C$ having quasi-maximal volume, $\hat{A}_1$. It will also yield the corresponding row-indices set
\begin{equation*}
  \I_1 = [I_1^{(\alpha_1)}], \quad \alpha_1 = 1,\ldots,r_1
\end{equation*}

Matrix $A_1$ is now represented as
\begin{equation*}
  A_1 = C\hat{A}_1^{-1}R
\end{equation*}
and similarly to the TT-SVD case, we can set the first tensor carriage as
\begin{equation*}
  G_1 = C\hat{A}_1^{-1}
\end{equation*}

We can now notice that, if $\hat{A}_1$ is of maximal volume in $C$, then $G_1$ has elements not higher than 1 in modulus \todo{Why is this true? Proof requred!}. This property on the elements of $G_1$ holds when $\hat{A}_1$ is the output of \emph{maxvol} \todo{add refs here, can be found at page 9 of tt-cross paper}.

The next step is to iterate further on the $R$ term and represent it in the TT format. $R$ consists of $r_1$ rows of $A_1$ and it can be reshaped into a subtensor $\tensor{R}$ of $\A$:
\begin{equation*}
  R(\alpha_1,i_2,\ldots,i_d) = A(i_1^{(\alpha_1)},i_2,\ldots,i_d)
\end{equation*}
We can now concatenate indices $\alpha_1$ and $i_2$ in one long index $\alpha_1i_2$: in this way $\tensor{R}$ can be seen as a tensor of dimensionality $d-1$ and size $r_1n_2 \times n_2 \times \cdots \times n_d$.

Similarly to as shown in proof of \ref{teo:ttsvd}, if the unfoldings $A_k$ have ranks $r_k$, then the corresponding compression ranks for $\tensor{R}$ cannot be larger and, in case the TT representation of $\A$ is exact, these compression ranks must be equal to $r_k$.\todo{Clarify and prove this.}

In order to obtain the second carriage, we consider again the first unfolding of the new, reshaped, subtensor \tensor{R}:
\begin{equation*}
  R_2 = R(\alpha_1i_2;i_3,\ldots,i_d)
\end{equation*}

%Page 79 of article
Now suppose that we know a set
\begin{equation*}
  \J_2 = [j_l^{(\alpha_2)}], \quad \alpha_2 = 1,\ldots,r_2, \quad l = 2,\ldots,d
\end{equation*}
of $(d-2)$-tuples, indicating the columns containing a submatrix of quasi-maximal volume in $R_2$. In order to obtain the skeleton decomposition of $R_2$, we use these linearly independent \todo{Is it proven that columns containing a quasi-maximal submatrix are linearly independent?} columns to make matrix $C_2$ of size $r_1 n_2 \times r_2$. Computing its entries
\begin{equation*}
  C_2(\alpha_1 i_2; \alpha_2) = R(\alpha_1 i_2,j_3^{(\alpha_2)},\ldots,,j_d^{(\alpha_2)})
\end{equation*}
that, in terms of the initial array are
\begin{equation*}
  C_2(\alpha_1 i_2; \alpha_2) = R(i_1^{(\alpha_1)},i_2,j_3^{(\alpha_2)},\ldots,j_d^{(\alpha_2)})
\end{equation*}
where the indices $i_1^{(\alpha_1)}$ belong to set $\I_1$ computed at the previous step.

As before, we find the rows containing a quasi-maximal volume submatrix \todo{Clarify the definition of a quasi-maximal volume submatrix of a matrix!} in $C_2$ and we construct the skeleton decomposition by using $C_2$ and some row matrix with $r_2$ rows. Again this row matrix gives rise to a new subtensor of size $r_2n_3 \times n_4 \times n_d$, which plays the same role and will be handled in the same way as $\tensor{R}$.

The row indices that determine the quasi-maximal volume submatrix in $C_2$ are chosen among the values of the long indices $\alpha_1i_2$ where $\alpha_1$ corresponds to \emph{one-dimensional} indices in $\I_1$. These can thus be considered as long indices of the form $i_1^{(\alpha_2)}i_2^{(\alpha_2)}$ and can be generalized straightforwardly to all further unfoldings.

At the $k$-th step we have a subtensor $\tensor{R}$ of dimensionality $d-k+1$ and size $r_{k-1}n_k \times n_{k+1} \times \ldots \times n_d$. Its definition as subset of entries of the initial tensor $\A$ is the following:
\begin{equation*}
  R(\alpha_{k-1}n_k,n_{k+1},\ldots,n_d) = A(i_1^{(\alpha_{k-1})},i_2^{(\alpha_{k-1})},\ldots,i_{k-1}^{(\alpha_{k-1})},i_k,i_{k+1},\ldots,i_d)
\end{equation*}
The first $k-1$ indices, which we call \emph{left indices}, are taken as $(k-1)$-tuples from the already computed entries of the \emph{left index set} \todo{???sicuramente le entries degli steps precedenti sono conosciute}
\begin{equation*}
  \I_{k-1} = [i_s^{(\alpha_{k-1})}],\quad \alpha_{k-1} = 1,\ldots,r_{k-1},\quad s = 1,\ldots,k-1
\end{equation*}
The first unfolding of the current subtensor $\tensor{R}$ at the $k$-th step is
\begin{equation*}
  R_k = [R(\alpha_{k-1}i_k;i_{k+1},\ldots,i_d)]
\end{equation*}
Again, columns containing a submatrix of quasi-maximal volume are supposed to be known and taken from the set of $(d-k)$-tuples
\begin{equation*}
  \J_k = [j_l^{(\alpha_k)}],\quad \alpha_k = 1,\ldots,r_k,\quad l = k+1,\ldots,d
\end{equation*}
Obviously these columns contain a submatrix $C_k$ of size $r_{k-1}n_k \times r_k$ with entries
\begin{equation*}
  C_k(\alpha_{k-1}i_k,\alpha_k) = A(i_1^{(\alpha_{k-1})},i_2^{(\alpha_{k-1})},\ldots,i_{k-1}^{(\alpha_{k-1})},i_k,j_{k+1}^{(\alpha_k)},\ldots,j_d^{(\alpha_k)})
\end{equation*}
Now the next \emph{left index set} is defined by the row positions of the quasi-maximal volume submatrix in $C_k$, which we can now denote as $\hat{R}^{-1}$.

The new tensor carriage $\G_k$ is then obtained by reshaping the result of $C_k \hat{R}^{-1}$

Let's now recap the entire procedure. If we know the index sets $\I_k$
\begin{equation*}
  \J_k = [j_l^{(\alpha_k)}],\quad \alpha_k = 1,\ldots,r_k,\quad l = k+1,\ldots,d
\end{equation*}
%EOF
