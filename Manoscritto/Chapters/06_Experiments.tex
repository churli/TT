\chapter{Experimental results}
We conclude this work by presenting some examples of both compression from \emph{full} format to TT and TT cross approximation. The tensors used are built by evaluating very simple multivariate functions on a mesh-grid. The purpose of this is to get an idea on how much the TT representation is able to decrease the number of parameters involved in representing an approximate tensor.

\paragraph{TT-SVD.}
For what concerns TT-SVD, we have considered the evenly-spaced $n~\times~\cdots~\times~n$ grid on $[-\pi,\pi\,]^d$. We have then explicitly built the full tensor by computing the function on the $n^d$ points of the grid and eventually we have applied the TT-SVD algorithm to get the TT representation of the tensor.

The functions we considered are
\begin{align*}
  f(x_1,\ldots,x_d) &= \sum_{i=1}^{d} x_i\\
  g(x_1,\ldots,x_d) &= sin(\sum_{i=1}^{d} x_i)\\
  h(x_1,\ldots,x_d) &= \sum_{i=1}^{d} \sqrt{|x_i|}\\
\end{align*}
and $n$ was equal to $10$, $d$ ranged from $3$ to $7$ and $\epsilon$ was $1^{-10}$.
The results are shown in Tables \ref{tab:ttsvd-sum}, \ref{tab:ttsvd-sinsum} and \ref{tab:ttsvd-sumsqrtabs}.

\begin{center}
  \begin{tabular}[ht]{cccccrr}
    \toprule
    d & n  & Size full & Size TT & Size ratio & Computation time & TT-SVD time\\ \midrule
    3 & 10 & 1\,000     & 60     & 0.06      & 0 ms & 4 ms\\
    4 & 10 & 10\,000    & 100    & 0.01      & 1 ms & 7 ms\\
    5 & 10 & 100\,000   & 520    & 0.0052    & 26 ms & 54 ms\\
    6 & 10 & 1\,000\,000  & 680    & 0.00068   & 45 ms & 96 ms\\ 
    7 & 10 & 10\,000\,000 &840&8.4e-05& 623 ms & 1\,259 ms\\ \bottomrule
  \end{tabular}
  \captionof{table}{\label{tab:ttsvd-sum}Results of using TT-SVD algorithm on $f(x_1,\ldots,x_d) = \sum_{i=1}^{d} x_i$.}
\end{center}

\begin{center}
  \begin{tabular}[ht]{cccccrr}
    \toprule
    d & n  & Size full & Size TT & Size ratio & Computation time & TT-SVD time\\ \midrule
    3 & 10 & 1\,000      &60&0.06 & 1 ms & 1 ms\\
    4 & 10 & 10\,000     &100&0.01 & 4 ms & 6 ms\\
    5 & 10 & 100\,000    &520&0.0052 & 9 ms & 9 ms\\
    6 & 10 & 1\,000\,000   &680&0.00068 & 84 ms & 100 ms\\
    7 & 10 & 10\,000\,000  &840&8.4e-05& 1\,354 ms & 1\,249 ms\\ \bottomrule
  \end{tabular}
  \captionof{table}{\label{tab:ttsvd-sinsum}Results of using TT-SVD algorithm on $g(x_1,\ldots,x_d) = sin(\sum_{i=1}^{d} x_i)$.}
\end{center}

\begin{center}
  \begin{tabular}[ht]{cccccrr}
    \toprule
    d & n  & Size full & Size TT & Size ratio & Computation time & TT-SVD time\\ \midrule
    3 & 10 & 1\,000      &60&0.06& 0 ms & 1 ms\\
    4 & 10 & 10\,000     &100&0.01& 1 ms & 6 ms\\
    5 & 10 & 100\,000    &300&0.003& 2 ms & 7 ms\\
    6 & 10 & 1\,000\,000   &390&0.00039& 52 ms & 75 ms\\
    7 & 10 & 10\,000\,000  &800&8e-05& 1\,587 ms & 1\,037 ms\\ \bottomrule
  \end{tabular}
  \captionof{table}{\label{tab:ttsvd-sumsqrtabs}Results of using TT-SVD algorithm on $h(x_1,\ldots,x_d) = \sum_{i=1}^{d} \sqrt{|x_i|}$.}
\end{center}

From these results we can clearly see how the number of parameters involved does not grow exponentially in $d$ and it can be comparatively very small in order to maintain the prescribed accuracy.

%Cross
\paragraph{TT-Cross.}
For what concerns TT-Cross, we have again considered the evenly-spaced $n~\times~\cdots~\times~n$ grid on $[-\pi,\pi\,]^d$. We have then invoked the TT-Cross algorithm, using the simple rank-one tensor $T_0(i_1,\ldots,i_d) = 1 \quad \forall i_1,\ldots,i_d$ as a first approximation and the chosen multidimensional function for evaluating the tensor on the relevant grid points.

Since in this case we does not have to build or store any tensor in the full format, we can push the dimensionality much higher (up to $d = 200$ in our case), while still being able to keep the tensor in memory and to manipulate it.

The functions we considered in this case are
\begin{align*}
  f(x_1,\ldots,x_d) &= \sum_{i=1}^{d} x_i\\
  g(x_1,\ldots,x_d) &= sin(\sum_{i=1}^{d} x_i)\\
\end{align*}
and $n$ was equal to $10$, $d$ ranged from $10$ to $200$ and $\epsilon$ was $1^{-10}$.
The results are shown in Table \ref{tab:ttcross}.

\begin{center}
  \begin{tabular}[ht!]{ccccrcr}
    \toprule
      %&    &           & \multicolumn{2}{c}{$\sum_{i=1}^{d} x_i$} & \multicolumn{2}{c}{$sin(\sum_{i=1}^{d} x_i)$}\\
      &    &           & \multicolumn{2}{c}{$sum(x_i)$} & \multicolumn{2}{c}{$sin(sum(x_i))$}\\
    d & n  & Size full & Size TT & TT-Cross time & Size TT & TT-Cross time\\ \midrule
    10  &10   &$10^{10}$      &6\,930     & 44 ms    &23\,100    & 139 ms\\
    25  &10   &$10^{25}$      &18\,820    & 206 ms    &63\,020    & 752 ms\\
    50  &10   &$10^{50}$      &40\,780    & 776 ms    &129\,610   &2\,974 ms\\
    100 &10   &$10^{100}$     &82\,860    &3\,074 ms    &290\,120   &12\,525 ms\\
    200 &10   &$10^{200}$     &165\,710   &11\,737 ms    &561\,790   &47\,023 ms\\ \bottomrule
  \end{tabular}
  \captionof{table}{\label{tab:ttcross}Results of using TT-Cross algorithm on $\sum_{i=1}^{d} x_i$ and on $sin(\sum_{i=1}^{d} x_i)$ with high dimension and $\epsilon = 1^{-10}$.}
\end{center}

Again, the results are very clear.
The fact that we are able, on a normal workstation, to aproximate and handle a $10^{10}$ tensor is quite remarkable on its own. However we can see how easy it is to work on the tensor up to $d = 200$, which would clearly be unfeasible in the full format.

%eof
