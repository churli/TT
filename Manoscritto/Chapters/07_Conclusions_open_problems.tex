\chapter{Applications and conclusions}
We have described this recently developed tensor representation format which can be used for effective low-parametric approximation.
The TT-format uses a compact and clean representation model, is based entirely on QR and SVD representation of matrices -- and skeleton decomposition in case of the TT-cross algorithm -- and it has a fast and stable rounding procedure.
This simplicity is also reflected in its algorithms: the code is short, doesn't require any recursion and basic linear algebra operations have an intuitive and fast implementation.

The TT-format has gained interest and has shown results in fields where there is the need for solving high-dimensional problems. Works have been published showing its application in partial differential equations \cite{khoromskij2011qtt}, high-dimensional elliptic equations and elliptic equations with variable coefficients \cite{dolgov2010tensor}, quantum molecular dynamics \cite{khoromskij2010dmrg+}.

Methods for solving linear systems and perform matrix inversion leveraging the TT-format have also been shown \cite{oseledets2012solution}, being able to treat problems with $2^d$ unknowns, where d is in the order of several hundreds. To put this in perspective, the number of atoms in the observable universe is commonly\footnote{A simple estimation method can be found at \href{https://en.wikipedia.org/wiki/Observable_universe}{https://en.wikipedia.org/wiki/Observable\_universe}} estimated at around $10^{80} \approx 2^{268}$ and being able to represent and solve a linear system with a number of unknown in the same order of magnitude is a pretty remarkable result.

Also, in an era where \emph{AI}\footnote{Artificial Intelligence} is seeing dramatic improvements, is getting widespread and is increasingly generating huge, high-dimensional data sets, we can expect the TT-format to be at the heart of new memory-efficient methods of representing and handling such data.

Memory access, data replication and transferability have become the main bottleneck in high performance and parallel computing: being able to remove the exponential dependency between high-dimensionality and its memory footprint will make it possible to have major performance improvements in real world applications.

%EOF
